{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',  # Identifie le navigateur\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',  # Accepte plusieurs types de contenu\n",
    "    'Accept-Encoding': 'gzip, deflate, br',  # Gère la compression des données\n",
    "    'Accept-Language': 'en-US,en;q=0.9,fr;q=0.8',  # Précise les langues acceptées\n",
    "    'Connection': 'keep-alive',  # Maintient la connexion ouverte pour des requêtes multiples\n",
    "    'Referer': 'https://www.huffingtonpost.fr/agriculture/',  # URL référente pour simuler une navigation depuis la page d'accueil\n",
    "    'Origin': 'https://www.huffingtonpost.fr',  # L'origine de la requête pour plus de légitimité\n",
    "    'Cache-Control': 'max-age=0',  # Assure que le cache ne soit pas utilisé pour cette requête\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_page(url):\n",
    "    all_articles = []  # Liste des articles collectés\n",
    "    seen_articles = set()  # Ensemble pour éviter les doublons d'URL\n",
    "\n",
    "    try:\n",
    "        # Envoyer une requête HTTP pour récupérer la page\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Trouver tous les articles via la classe \"articlePreview-content\"\n",
    "            articles = soup.find_all(\"div\", class_=\"articlePreview-content\")\n",
    "            \n",
    "            for article in articles:\n",
    "                # Extraire le lien de l'article\n",
    "                link_element = article.find(\"a\", href=True)\n",
    "                article_url = f\"https://www.huffingtonpost.fr{link_element['href']}\" if link_element else \"\"\n",
    "\n",
    "                # Extraire le titre\n",
    "                title_element = article.find(\"div\", class_=\"title\")\n",
    "                title_text = title_element.find(\"h2\").get_text(strip=True) if title_element else \"\"\n",
    "\n",
    "                # Extraire la description\n",
    "                description_element = article.find(\"div\", class_=\"articlePreview-chapo\")\n",
    "                description_text = description_element.get_text(strip=True) if description_element else \"\"\n",
    "\n",
    "                # Vérifier si l'article est déjà collecté\n",
    "                if article_url and article_url not in seen_articles:\n",
    "                    all_articles.append([title_text, description_text, article_url])\n",
    "                    seen_articles.add(article_url)\n",
    "\n",
    "            return all_articles\n",
    "\n",
    "        else:\n",
    "            print(f\"Erreur HTTP {response.status_code} lors de l'accès à {url}\")\n",
    "            return []\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Erreur de requête pour {url}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_pages(base_url, total_pages):\n",
    "    all_articles = []  # Liste pour collecter tous les articles\n",
    "\n",
    "    for page_num in range(1, total_pages + 1):\n",
    "        # Construire l'URL pour chaque page\n",
    "        if page_num == 1:\n",
    "            url = base_url  # La première page n'a pas de suffixe \"?page=\"\n",
    "        else:\n",
    "            url = f\"{base_url}?page={page_num}\"\n",
    "        \n",
    "        print(f\"Scraping de la page : {url}\")\n",
    "        articles = scrape_all_page(url)  # Appeler la fonction pour une page\n",
    "        all_articles.extend(articles)  # Ajouter les articles collectés\n",
    "\n",
    "        print(f\"Page {page_num} : {len(articles)} articles collectés.\")\n",
    "\n",
    "    print(f\"Scraping terminé : {len(all_articles)} articles collectés au total.\")\n",
    "    return all_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour exporter les résultats en CSV\n",
    "def export_to_csv_2(dataset, filename=\"filtered_articles.csv\"):\n",
    "    with open(filename, mode='w', encoding='utf-8', newline='') as file:\n",
    "        fieldnames = [\"Title\", \"Summary\", \"Date\", \"Paragraphs\"]\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for data in dataset:\n",
    "            # Convertir la liste de paragraphes en une seule chaîne avec des doubles sauts de ligne\n",
    "            data[\"Paragraphs\"] = \"\\n\\n\".join(data[\"Paragraphs\"])\n",
    "            writer.writerow(data)\n",
    "\n",
    "    print(f\"Les résultats ont été sauvegardés dans '{filename}'.\")\n",
    "\n",
    "def export_to_csv(dataset, filename=\"articles_dataset.csv\"):\n",
    "    # Ouvrir le fichier CSV en mode écriture\n",
    "    with open(filename, mode='w', encoding='utf-8', newline='') as file:\n",
    "        # Définir les noms des colonnes (en-têtes)\n",
    "        fieldnames = [\"Title\", \"Summary\", \"URL\"]\n",
    "        \n",
    "        # Créer un objet DictWriter pour écrire dans le fichier\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        \n",
    "        # Écrire les en-têtes dans le fichier\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Écrire chaque ligne de données dans le fichier\n",
    "        for data in dataset:\n",
    "            writer.writerow(data)\n",
    "\n",
    "    print(f\"Les résultats ont été sauvegardés dans '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/\n",
      "Page 1 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=2\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=2\n",
      "Page 2 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=3\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=3\n",
      "Page 3 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=4\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=4\n",
      "Page 4 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=5\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=5\n",
      "Page 5 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=6\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=6\n",
      "Page 6 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=7\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=7\n",
      "Page 7 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=8\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=8\n",
      "Page 8 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=9\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=9\n",
      "Page 9 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=10\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=10\n",
      "Page 10 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=11\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=11\n",
      "Page 11 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=12\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=12\n",
      "Page 12 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=13\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=13\n",
      "Page 13 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=14\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=14\n",
      "Page 14 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=15\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=15\n",
      "Page 15 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=16\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=16\n",
      "Page 16 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=17\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=17\n",
      "Page 17 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=18\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=18\n",
      "Page 18 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=19\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=19\n",
      "Page 19 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=20\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=20\n",
      "Page 20 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=21\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=21\n",
      "Page 21 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=22\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=22\n",
      "Page 22 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=23\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=23\n",
      "Page 23 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=24\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=24\n",
      "Page 24 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=25\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=25\n",
      "Page 25 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=26\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=26\n",
      "Page 26 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=27\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=27\n",
      "Page 27 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=28\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=28\n",
      "Page 28 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=29\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=29\n",
      "Page 29 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=30\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=30\n",
      "Page 30 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=31\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=31\n",
      "Page 31 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=32\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=32\n",
      "Page 32 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=33\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=33\n",
      "Page 33 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=34\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=34\n",
      "Page 34 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=35\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=35\n",
      "Page 35 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=36\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=36\n",
      "Page 36 : 0 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=37\n",
      "Erreur HTTP 406 lors de l'accès à https://www.huffingtonpost.fr/agriculture/?page=37\n",
      "Page 37 : 0 articles collectés.\n",
      "Scraping terminé : 0 articles collectés au total.\n",
      "Les résultats ont été sauvegardés dans 'agriculture_huffpost.csv'.\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.huffingtonpost.fr/agriculture/\"\n",
    "total_pages = 37\n",
    "\n",
    "# Scraper tous les articles des 37 pages\n",
    "all_articles = scrape_all_pages(base_url, total_pages)\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier CSV\n",
    "export_to_csv([{\"Title\": title, \"Summary\": desc, \"URL\": url} for title, desc, url in all_articles], \"agriculture_huffpost.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les informations d'un article\n",
    "def fetch_article_details(article_data, headers):\n",
    "    title, summary, url = article_data\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extraire le titre\n",
    "            title_tag = soup.find('h1', class_='article__title')\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"Titre non trouvé\"\n",
    "\n",
    "            # Extraire la description (résumé)\n",
    "            summary_tag = soup.find('p', class_='article__desc')\n",
    "            summary = summary_tag.get_text(strip=True) if summary_tag else \"Résumé non trouvé\"\n",
    "\n",
    "            # Extraire la date\n",
    "            date_tag = soup.find(\"span\", class_=\"article-metas__date\")\n",
    "            date = date_tag.get_text(strip=True) if date_tag else \"Date non trouvée\"\n",
    "\n",
    "            # Extraire les paragraphes à partir de la classe 'article__paragraph'\n",
    "            paragraphs = soup.find_all('p', class_='article__paragraph')\n",
    "            paragraph_list = [p.get_text(strip=True) for p in paragraphs]\n",
    "\n",
    "            return {\n",
    "                \"Title\": title,\n",
    "                \"Summary\": summary,\n",
    "                \"Date\": date,\n",
    "                \"Paragraphs\": paragraph_list\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Erreur HTTP {response.status_code} pour {url}\")\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Erreur de requête pour {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour vérifier si \"bio\" est dans le texte\n",
    "def contains_bio(text):\n",
    "    return 'bio' in text.lower()\n",
    "\n",
    "# Fonction pour filtrer les articles contenant le mot \"bio\"\n",
    "def filter_articles_with_bio(dataset):\n",
    "    filtered_articles = []\n",
    "    total_articles = len(dataset)\n",
    "    \n",
    "    # Initialiser tqdm pour la boucle avec un calcul du temps estimé\n",
    "    start_time = time.time()  # Démarrer le chronomètre\n",
    "    \n",
    "    for i, article in tqdm(enumerate(dataset), total=total_articles, desc=\"Filtrage des articles\", unit=\"article\"):\n",
    "        # Vérifier si le mot \"bio\" est dans le titre ou le résumé\n",
    "        if contains_bio(article[\"Title\"]) or contains_bio(article[\"Summary\"]):\n",
    "            filtered_articles.append(article)\n",
    "        else:\n",
    "            # Vérifier chaque paragraphe si le mot \"bio\" est présent\n",
    "            for paragraph in article[\"Paragraphs\"]:\n",
    "                if contains_bio(paragraph):\n",
    "                    filtered_articles.append(article)\n",
    "                    break\n",
    "        \n",
    "        # Calculer et afficher le temps estimé restant\n",
    "        elapsed_time = time.time() - start_time  # Temps écoulé\n",
    "        progress = (i + 1) / total_articles  # Progression\n",
    "        estimated_time_left = elapsed_time / progress - elapsed_time  # Temps estimé restant\n",
    "        print(f\"\\rProgression : {progress * 100:.2f}% - Temps restant estimé : {estimated_time_left / 60:.2f} minutes\", end=\"\")\n",
    "    \n",
    "    print()  # Pour un retour à la ligne après l'affichage du progrès\n",
    "    return filtered_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtrage des articles: 0article [00:00, ?article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Les résultats ont été sauvegardés dans 'bio_huffpost.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Scraper les détails de chaque article dans `all_articles`\n",
    "detailed_articles = []\n",
    "\n",
    "for article_data in all_articles:\n",
    "    details = fetch_article_details(article_data, headers)\n",
    "    if details:  # Si les détails sont récupérés avec succès\n",
    "        detailed_articles.append(details)\n",
    "\n",
    "# Filtrer les articles contenant le mot \"bio\"\n",
    "filtered_articles = filter_articles_with_bio(detailed_articles)\n",
    "\n",
    "# Exporter les articles filtrés en CSV\n",
    "export_to_csv_2(filtered_articles, \"bio_huffpost.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

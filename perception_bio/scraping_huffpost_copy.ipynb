{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",  # Supporte la compression\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",  # Pour indiquer le support de HTTPS et les mises à jour\n",
    "    \"Cache-Control\": \"max-age=0\",  # Pour forcer le rechargement des ressources\n",
    "    \"TE\": \"Trailers\"  # Support de la transmission avec les trailers HTTP/2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_page(url):\n",
    "    all_articles = []  # Liste des articles collectés\n",
    "    seen_articles = set()  # Ensemble pour éviter les doublons d'URL\n",
    "\n",
    "    try:\n",
    "        # Envoyer une requête HTTP pour récupérer la page\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Trouver tous les articles via la classe \"articlePreview-content\"\n",
    "            articles = soup.find_all(\"div\", class_=\"articlePreview-content\")\n",
    "            \n",
    "            for article in articles:\n",
    "                # Extraire le lien de l'article\n",
    "                link_element = article.find(\"a\", href=True)\n",
    "                article_url = f\"https://www.huffingtonpost.fr{link_element['href']}\" if link_element else \"\"\n",
    "\n",
    "                # Extraire le titre\n",
    "                title_element = article.find(\"div\", class_=\"title\")\n",
    "                title_text = title_element.find(\"h2\").get_text(strip=True) if title_element else \"\"\n",
    "\n",
    "                # Extraire la description\n",
    "                description_element = article.find(\"div\", class_=\"articlePreview-chapo\")\n",
    "                description_text = description_element.get_text(strip=True) if description_element else \"\"\n",
    "\n",
    "                # Vérifier si l'article est déjà collecté\n",
    "                if article_url and article_url not in seen_articles:\n",
    "                    # Ajouter l'article sous forme de dictionnaire\n",
    "                    all_articles.append({\n",
    "                        \"title\": title_text,\n",
    "                        \"description\": description_text,\n",
    "                        \"url\": article_url\n",
    "                    })\n",
    "                    seen_articles.add(article_url)\n",
    "\n",
    "            return all_articles\n",
    "\n",
    "        else:\n",
    "            print(f\"Erreur HTTP {response.status_code} lors de l'accès à {url}\")\n",
    "            return []\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Erreur de requête pour {url}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_pages(base_url, total_pages):\n",
    "    all_articles = []  # Liste pour collecter tous les articles\n",
    "\n",
    "    for page_num in range(1, total_pages + 1):\n",
    "        # Construire l'URL pour chaque page\n",
    "        if page_num == 1:\n",
    "            url = base_url  # La première page n'a pas de suffixe \"?page=\"\n",
    "        else:\n",
    "            url = f\"{base_url}?page={page_num}\"\n",
    "        \n",
    "        print(f\"Scraping de la page : {url}\")\n",
    "        articles = scrape_all_page(url)  # Appeler la fonction pour une page\n",
    "        all_articles.extend(articles)  # Ajouter les articles collectés\n",
    "\n",
    "        print(f\"Page {page_num} : {len(articles)} articles collectés.\")\n",
    "\n",
    "    print(f\"Scraping terminé : {len(all_articles)} articles collectés au total.\")\n",
    "    return all_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour exporter les résultats en CSV\n",
    "def export_to_csv_1(dataset, filename=\"articles_dataset.csv\"):\n",
    "    # Ouvrir le fichier CSV en mode écriture\n",
    "    with open(filename, mode='w', encoding='utf-8', newline='') as file:\n",
    "        # Définir les noms des colonnes (en-têtes)\n",
    "        fieldnames = [\"Title\", \"Description\", \"URL\"]\n",
    "        \n",
    "        # Créer un objet DictWriter pour écrire dans le fichier\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        \n",
    "        # Écrire les en-têtes dans le fichier\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Écrire chaque ligne de données dans le fichier\n",
    "        for data in dataset:\n",
    "            # Formatage des données pour correspondre aux en-têtes\n",
    "            writer.writerow({\n",
    "                \"Title\": data[\"title\"],\n",
    "                \"Description\": data[\"description\"],\n",
    "                \"URL\": data[\"url\"]\n",
    "            })\n",
    "\n",
    "    print(f\"Les résultats ont été sauvegardés dans '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=2\n",
      "Page 2 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=3\n",
      "Page 3 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=4\n",
      "Page 4 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=5\n",
      "Page 5 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=6\n",
      "Page 6 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=7\n",
      "Page 7 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=8\n",
      "Page 8 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=9\n",
      "Page 9 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=10\n",
      "Page 10 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=11\n",
      "Page 11 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=12\n",
      "Page 12 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=13\n",
      "Page 13 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=14\n",
      "Page 14 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=15\n",
      "Page 15 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=16\n",
      "Page 16 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=17\n",
      "Page 17 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=18\n",
      "Page 18 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=19\n",
      "Page 19 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=20\n",
      "Page 20 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=21\n",
      "Page 21 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=22\n",
      "Page 22 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=23\n",
      "Page 23 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=24\n",
      "Page 24 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=25\n",
      "Page 25 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=26\n",
      "Page 26 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=27\n",
      "Page 27 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=28\n",
      "Page 28 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=29\n",
      "Page 29 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=30\n",
      "Page 30 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=31\n",
      "Page 31 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=32\n",
      "Page 32 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=33\n",
      "Page 33 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=34\n",
      "Page 34 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=35\n",
      "Page 35 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=36\n",
      "Page 36 : 30 articles collectés.\n",
      "Scraping de la page : https://www.huffingtonpost.fr/agriculture/?page=37\n",
      "Page 37 : 22 articles collectés.\n",
      "Scraping terminé : 1102 articles collectés au total.\n",
      "Les résultats ont été sauvegardés dans 'agriculture_huffpost_.csv'.\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.huffingtonpost.fr/agriculture/\"\n",
    "total_pages = 37\n",
    "\n",
    "# Scraper tous les articles des 37 pages\n",
    "all_articles = scrape_all_pages(base_url, total_pages)\n",
    "\n",
    "# Sauvegarder les résultats dans un fichier CSV\n",
    "export_to_csv_1(all_articles, \"agriculture_huffpost_.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'C’est quoi cet oiseau qui a mis un coup d’arrêt au projet de bassine à Sainte-Soline',\n",
       "  'description': 'Ce mercredi, la cour administrative d’appel de Bordeaux a jugé les autorisations délivrées à quatre réserves d’irrigation du Poitou, dont celle de Sainte-Soline, illégales.',\n",
       "  'url': 'https://www.huffingtonpost.fr/environnement/article/sainte-soline-l-outarde-canepetiere-l-oiseau-qui-a-mis-un-coup-d-arret-au-projet-de-bassine_243753.html'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article_detail(articles):\n",
    "    \"\"\"\n",
    "    Enrichit une liste d'articles avec les paragraphes et la date de publication.\n",
    "    \n",
    "    :param articles: Liste de dictionnaires contenant 'title', 'description' et 'url' pour chaque article.\n",
    "    :return: Liste enrichie avec les 'paragraphs' et 'publication_date'.\n",
    "    \"\"\"\n",
    "    articles_enrichis = []\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            # Extraction des informations de l'article\n",
    "            titre = article.get(\"title\")\n",
    "            resume = article.get(\"description\")\n",
    "            url = article.get(\"url\")\n",
    "\n",
    "            # Récupération de la page HTML\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Vérifie que la requête est réussie\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extraction des paragraphes\n",
    "            paragraphes = [\n",
    "                p.get_text(strip=True) for p in soup.find_all('p', class_='asset asset-text')\n",
    "            ]\n",
    "            \n",
    "            # Extraction de la date de publication\n",
    "            date_element = soup.find('time', class_='article-metas')\n",
    "            if date_element:\n",
    "                date_publication = date_element.find('span', class_='article-metas__date')\n",
    "                date_publication = date_publication.get_text(strip=True) if date_publication else \"Date inconnue\"\n",
    "            else:\n",
    "                date_publication = \"Date inconnue\"\n",
    "            \n",
    "            # Ajouter les données enrichies à l'article\n",
    "            articles_enrichis.append({\n",
    "                \"title\": titre,\n",
    "                \"description\": resume,\n",
    "                \"url\": url,\n",
    "                \"paragraphs\": paragraphes,\n",
    "                \"publication_date\": date_publication\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            # En cas d'erreur, log et continuer\n",
    "            print(f\"Erreur pour l'article : {titre} ({url}) - {e}\")\n",
    "            articles_enrichis.append({\n",
    "                \"title\": titre,\n",
    "                \"description\": resume,\n",
    "                \"url\": url,\n",
    "                \"paragraphs\": [],\n",
    "                \"publication_date\": \"Erreur lors de l'extraction\"\n",
    "            })\n",
    "    \n",
    "    return articles_enrichis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour vérifier si \"bio\" est dans le texte et retourner les phrases contenant \"bio\"\n",
    "def find_sentences_with_bio(text):\n",
    "    text = text.lower()\n",
    "    sentences = text.split('.')  # Diviser le texte en phrases\n",
    "    bio_sentences = [sentence.strip() for sentence in sentences if 'bio' in sentence or 'biologique' in sentence]\n",
    "    return bio_sentences\n",
    "\n",
    "# Fonction pour filtrer les articles contenant le mot \"bio\" et ajouter les phrases correspondantes\n",
    "def filter_articles_with_bio(dataset):\n",
    "    filtered_articles = []\n",
    "    total_articles = len(dataset)\n",
    "\n",
    "    # Initialiser tqdm pour la boucle avec un calcul du temps estimé\n",
    "    start_time = time.time()  # Démarrer le chronomètre\n",
    "\n",
    "    for i, article in tqdm(enumerate(dataset), total=total_articles, desc=\"Filtrage des articles\", unit=\"article\"):\n",
    "        bio_sentences = []\n",
    "\n",
    "        # Vérifier si \"bio\" est dans le titre\n",
    "        bio_sentences += find_sentences_with_bio(article.get(\"title\", \"\"))\n",
    "\n",
    "        # Vérifier si \"bio\" est dans le résumé\n",
    "        bio_sentences += find_sentences_with_bio(article.get(\"description\", \"\"))\n",
    "\n",
    "        # Vérifier chaque paragraphe\n",
    "        for paragraph in article.get(\"paragraphs\", []):\n",
    "            bio_sentences += find_sentences_with_bio(paragraph)\n",
    "\n",
    "        if bio_sentences:  # Ajouter seulement si des phrases contenant \"bio\" ont été trouvées\n",
    "            article_with_bio = article.copy()\n",
    "            article_with_bio[\"bio_sentences\"] = bio_sentences  # Ajouter les phrases contenant \"bio\"\n",
    "            filtered_articles.append(article_with_bio)\n",
    "\n",
    "        # Calculer et afficher le temps estimé restant\n",
    "        elapsed_time = time.time() - start_time  # Temps écoulé\n",
    "        progress = (i + 1) / total_articles  # Progression\n",
    "        estimated_time_left = elapsed_time / progress - elapsed_time  # Temps estimé restant\n",
    "        print(f\"\\rProgression : {progress * 100:.2f}% - Temps restant estimé : {estimated_time_left / 60:.2f} minutes\", end=\"\")\n",
    "\n",
    "    print()  # Pour un retour à la ligne après l'affichage du progrès\n",
    "    return filtered_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraper les détails de chaque article dans `all_articles`\n",
    "detailed_articles = fetch_article_detail(all_articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'C’est quoi cet oiseau qui a mis un coup d’arrêt au projet de bassine à Sainte-Soline',\n",
       "  'description': 'Ce mercredi, la cour administrative d’appel de Bordeaux a jugé les autorisations délivrées à quatre réserves d’irrigation du Poitou, dont celle de Sainte-Soline, illégales.',\n",
       "  'url': 'https://www.huffingtonpost.fr/environnement/article/sainte-soline-l-outarde-canepetiere-l-oiseau-qui-a-mis-un-coup-d-arret-au-projet-de-bassine_243753.html',\n",
       "  'paragraphs': ['ENVIRONNEMENT - C’est un oiseau qui contrarie les plans des agriculteurs du Poitou favorables auxmégabassines. Ce mercredi 18\\xa0décembre, quatre d’entre elles –notamment celle de Sainte-Soline– ont vu leurautorisation suspenduepar la justice. Pour cause, ces réserves d’irrigation sont «de nature à détruire tout ou une partie de l’habitat\\xa0»de l’outarde canepetière, d’après la cour administrative d’appel de Bordeaux.\\xa0Dans cette région, la population de cesoiseauxn’est estimée qu’à quelques centaines, menacés parl’agriculture intensive.',\n",
       "   'Selon les informations de laLPO, le Centre Ouest, notamment les Deux-Sèvres, accueille 327\\xa0mâles d’outarde canepetière. Pas évidente à repérer et craintive des humains, elle possède un plumage ocre avec des taches noires. Au printemps, pour la saison de la reproduction, le mâle arbore une collerette noire et blanche,«\\xa0qu’il déploie comme un cobra au cours de sa parade\\xa0», décrieReporterre. Son nom vient de«\\xa0cane\\xa0»pour son vol qui ressemble à celui d’un canard, et «petière\\xa0»pour les bruits aigus émis au moment de la parade, qui ressemblent à de petits pets.',\n",
       "   'Il s’agit de la dernière population migratoire de cette espèce en Europe. Elle se reproduit en majorité dans les plaines céréalières, qui sont placées en Zones de protection spéciales (ZPS), visant à assurer leur survie à long terme.',\n",
       "   'D’après Étienne Debenest, ornithologue au sein du Groupe ornithologique des Deux-Sèvres (GODS) rencontré parReporterre, l’outarde migre de moins en moins vers l’Espagne et le Portugal, comme auparavant.«\\xa0Avec le\\xa0réchauffement climatique, les hivers chez nous sont moins rudes. La végétation et le climat lui restent plus favorables\\xa0», observe le professionnel.',\n",
       "   'L’outarde canepetière est particulièrement menacée par l’agriculture intensive, explique la LPO, notamment avec la disparition des jachères et l’usage des pesticides. L’urbanisation et les infrastructures comme les parcs photovoltaïques constituent également un danger.',\n",
       "   'Ces deux facteurs entraînent des difficultés à trouver de la nourriture pour le jeune outardeau, qui consomme en moyenne«\\xa0200\\xa0insectes par jour\\xa0: grillons, criquets et autres sauterelles\\xa0», indiqueReporterre.«\\xa0Le contexte agricole et la quantité d’insectes dans nos plaines ne sont plus favorables et suffisants pour que l’outarde élève tous ses jeunes\\xa0», déplore Étienne Debenest. Les fauches et les phénomènes climatiques extrêmes menacent aussi l’éclosion des œufs, précise-t-il.',\n",
       "   'C’est pourquoi l’espèce est sous le coup d’un\\xa0troisième Plan National d’Actions (PNA). Ce dernier vise à éviter l’extinction de ces populations. L’oiseau est aussi classé parmi les espèces en danger dans la liste rouge de l’Union Internationale pour la Conservation de la Nature.',\n",
       "   'Une situation qui explique que l’outarde canepetière soit devenue l’oiseau totem des anti-bassines, qui protestent contre le modèle agricole intensif qui porte atteinte à son habitat.«Une seule outarde repérée sur le terrain peut justifier qu’on attaque en justice et même permettre de gagner contre un projet de bassine\\xa0»,assure l’ornithologue.',\n",
       "   'À voir également surLe HuffPost:'],\n",
       "  'publication_date': '18/12/2024 18:40'},\n",
       " {'title': 'La justice déclare illégale la bassine controversée de Sainte-Soline, mais...',\n",
       "  'description': 'En plus de Sainte-Soline, la justice a également jugé illégales les autorisations délivrées à trois autres mégabassines du Poitou ce mercredi.',\n",
       "  'url': 'https://www.huffingtonpost.fr/justice/article/sainte-soline-la-justice-juge-cette-bassine-controversee-des-deux-sevres-illegale_243746.html',\n",
       "  'paragraphs': ['JUSTICE - Cette décision survient après une longue lutte d’associations environnementales. Alors que les images de manifestants affrontant les forces de l’ordre àSainte-Soline, dans les Deux-Sèvres, avaient fait le tour des médias en 2023, ce mercredi 18\\xa0décembre, la cour administrative d’appel de Bordeaux a jugé illégales les autorisations délivrées à quatreréserves d’irrigation du Poitou, dont celle de Sainte-Soline. Selon la justice, cesmégabassinesmenacent la survie d’une espèce d’oiseau protégée, l’outarde canepetière.',\n",
       "   '«\\xa0L’eau stockée\\xa0»à la date de décision dans la réserve de Sainte-Soline, la seule des quatre dont la construction est achevée, pourra néanmoins être utilisée cet été «par les agriculteurs raccordés\\xa0», sans toutefois «donner lieu à un nouveau remplissage\\xa0», a précisé la cour.',\n",
       "   'Une dizaine d’associations environnementales avaient attaqué les autorisations délivrées par l’État pour construire et exploiter 16 retenues d’eau à usage agricole dans le Marais poitevin, dénommées «bassines\\xa0»par les opposants au projet. Dans la zone de leur implantation, le projet est«\\xa0de nature à détruire tout ou partie de cette espèce et lui porte une atteinte caractérisée\\xa0», écrit la cour dans un communiqué.',\n",
       "   'Quatre des seize bassines du projet sont «de nature à détruire tout ou une partie de l’habitat\\xa0»de l’outarde canepetière. La Cour juge donc que«\\xa0l’autorisation délivrée est illégale faute de prévoir une “dérogation espèces protégées’’\\xa0»et la suspend«\\xa0jusqu’à la délivrance éventuelle de cette dérogation\\xa0».',\n",
       "   'Les requérants - Nature Environnement 17, la Ligue de protection des oiseaux et des fédérations de pêcheurs - déboutés en première instance à Poitiers, reprochaient aussi au projet de réserves dites de substitution, que l’on remplit l’hiver en pompant dans les nappes afin de pouvoir irriguer en été, de nuire à l’équilibre de la ressource en eau.',\n",
       "   'La cour a rejeté ces accusations, considérant que «l’administration a conditionné le niveau de remplissage des réserves au respect de seuils pertinents fixés au regard du niveau de la nappe\\xa0»et que le projet «ne méconnaît pas le principe d’une gestion équilibrée et durable de l’eau\\xa0».',\n",
       "   'À voir également surLe HuffPost:'],\n",
       "  'publication_date': '18/12/2024 16:04'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_articles[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtrage des articles: 100%|██████████| 1102/1102 [00:00<00:00, 13707.97article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progression : 100.00% - Temps restant estimé : 0.00 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les articles contenant le mot \"bio\"\n",
    "filtered_articles = filter_articles_with_bio(detailed_articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Charger explicitement le modèle de sentiment d'Hugging Face\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def analyze_sentiment_bio_with_model(text):\n",
    "    \"\"\"\n",
    "    Utilise un modèle open-source pour analyser le sentiment d'un texte concernant le bio.\n",
    "    Le score sera entre 0 (négatif) et 1 (positif).\n",
    "    \"\"\"\n",
    "    # Utiliser le modèle de sentiment pour prédire le sentiment\n",
    "    result = sentiment_analyzer(text)\n",
    "\n",
    "    # Le modèle retourne une liste avec un dictionnaire contenant les labels et scores\n",
    "    sentiment = result[0]\n",
    "    \n",
    "    # Convertir le label en score : 'LABEL_1' est positif, 'LABEL_0' est négatif\n",
    "    if sentiment['label'] == 'POSITIVE':\n",
    "        return sentiment['score']\n",
    "    else:\n",
    "        return 1 - sentiment['score']  # Retourner un score entre 0 et 1 pour un sentiment négatif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour appliquer l'analyse de sentiment à chaque article dans la liste\n",
    "def add_sentiment_score_to_articles(articles):\n",
    "    \"\"\"\n",
    "    Applique l'analyse de sentiment sur chaque phrase ajoutée à l'article\n",
    "    et ajoute la moyenne des scores de sentiment à chaque article.\n",
    "    \n",
    "    :param articles: Liste de dictionnaires contenant les articles enrichis avec des clés comme 'title', 'description', 'url', 'paragraphs', 'bio_sentences'.\n",
    "    :return: Liste de dictionnaires enrichis avec le score de sentiment moyen.\n",
    "    \"\"\"\n",
    "    for article in articles:\n",
    "        # Extraire les phrases contenant \"bio\" (clé 'bio_sentences')\n",
    "        sentences = article.get(\"bio_sentences\", [])\n",
    "\n",
    "        if sentences:  # Appliquer l'analyse de sentiment uniquement si des phrases contenant \"bio\" sont présentes\n",
    "            # Appliquer l'analyse de sentiment à chaque phrase\n",
    "            sentiment_scores = [analyze_sentiment_bio_with_model(sentence) for sentence in sentences]\n",
    "\n",
    "            # Calculer la moyenne des scores de sentiment\n",
    "            average_score = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
    "\n",
    "            # Ajouter le score de sentiment moyen au dictionnaire\n",
    "            article[\"sentiment_score\"] = average_score\n",
    "        else:\n",
    "            # Si aucune phrase contenant \"bio\" n'est trouvée, définir le score comme 0\n",
    "            article[\"sentiment_score\"] = 0\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = add_sentiment_score_to_articles(filtered_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Des agriculteurs de la «\\xa0Conf’\\xa0» interpellés, le syndicat dénonce un «\\xa02\\xa0poids, 2\\xa0mesures\\xa0»',\n",
       " 'description': 'Après l’action du troisième syndicat agricole à Paris jeudi, «\\xa0cinq paysans\\xa0» sont toujours en garde à vue, indique leur porte-parole au «\\xa0HuffPost\\xa0».',\n",
       " 'url': 'https://www.huffingtonpost.fr/politique/video/des-agriculteurs-de-la-confederation-paysanne-interpelles-elle-denonce-le-2-poids-2-mesures-par-rapport-a-la-fnsea_243206.html',\n",
       " 'paragraphs': ['AGRICULTURE - Avant leur arrestation, ils avaient déposé des bottes de paille et chanté«\\xa0Traders tremblez, les paysans reprennent leur blé\\xa0!\\xa0»Une centaine d’agriculteurs de laConfédération paysanne, le troisième syndicat agricole français, classé à gauche, ont manifesté jeudi 5\\xa0décembre devant leGrand Palais, à Paris, où se tenait une réunion européenne de grands acteurs du commerce des céréales. Cinq manifestants ont été interpellés et sont toujours en garde à vue, indique le syndicat, ce vendredi\\xa06, auHuffPost.',\n",
       "  'Cette manifestation, que notre reporter a suiviecomme vous pouvez le voir dans la vidéo en tête d’article, a eu lieu alors que s’ouvre en Uruguay le sommet duMercosur, où l’UE, par la main de la présidente de la Commission européenne Ursula von\\xa0der\\xa0Leyen, pourrait signer un accord de libre-échange dénoncé par toutes les organisations agricoles françaises.',\n",
       "  'Alors que le rassemblement parisien se déroulait sans heurts, des forces de l’ordre sont arrivées au bout d’une vingtaine de minutes.«\\xa0Laissez tomber, vous n’avez plus de ministre\\xa0!\\xa0»les ont interpellées des manifestants. La police a ensuite bloqué les paysans pendant plus d’une heure, donnant lieu à des bousculades, a rapporté une journaliste de l’AFP.',\n",
       "  'In fine, les forces de l’ordre, appuyées par la brigade de répression de l’action violente motorisée (Brav-M), ont procédé à cinq interpellations. L’arrestation brutale de l’un d’entre eux a été filmée et partagée sur les réseaux sociaux,comme vous pouvez le voir dans le tweet ci-dessous. L’homme en manteau rouge, traîné par terre, s’appelle Toussaint Lamy. Il est agriculteur à Cornot, en Haute-Saône, indiqueFrance 3.',\n",
       "  '«\\xa0Quatre paysans et une paysanne sont toujours en garde à vue ce matin\\xa0», précise ce vendredi Laurence Marandola, porte-parole de la\\xa0Confédération\\xa0paysanne, interrogée parLe HuffPost.',\n",
       "  'Celle qui est le visage du troisième syndicat agricole ressent de«\\xa0la colère\\xa0»alors que des membres de son mouvement ont subi «deuxnassesinterdites, un déferlement de violence policière». De plus, pour Laurence Marandola, qui lutte pour un revenu juste des paysans vivant sous le seuil de pauvreté, l’action des forces de l’ordre est«\\xa0la preuve d’un deux poids, deux mesures» alors que«\\xa0la FNSEA peut tout faire et malmener les services publics sans être inquiétée\\xa0».',\n",
       "  'La porte-parole n’est pas la seule à dénoncer ce «traitement de faveur» qui serait réservé la FNSEA. Des députés écologistes ont aussi fait part de leur colère àfranceinfoaprès l’interpellation des cinq agriculteurs. L’un d’eux, Charles Fournier, élu de Tours, fait remarquer à nos confrères\\xa0que d’un côté, les paysans de la Conf’ paysanne sont «nassés et brutalisés lorsqu’ils protestent contre la spéculation sur les céréales\\xa0», pendant que de l’autre, il n’y a«\\xa0aucune mesure de sanction quand la FNSEA déverse sa colère sur les grilles de l’Office français de la Biodiversité\\xa0».Le 18\\xa0novembre, laFDSEA de l’Oise, une section départementale de la FNSEA, avait muré avec des parpaings les locaux de cet organisme public.',\n",
       "  'Fin novembre, des agriculteurs franciliens de la FNSEA-JA avaient également fait ériger des murs symboliques devant l’Anses et l’Inrae et soudé les portes de l’Office français de la biodiversité (OFB) à Toulouse. Leur but\\xa0: dénoncer les «normes» environnementales et sanitaires qui, selon eux, entravent leur production.',\n",
       "  'Des bâtiments publics du Tarn-et-Garonne ont de nouveau été pris pour cibles ce jeudi soir. Une quinzaine d’agriculteurs, dont l’affiliation syndicale n’est pas connue, ont déversé des bennes de détritus et de céréales moisies près de la sous-préfecture et du commissariat de Castelsarrasin avant de se diriger vers les locaux de la Mutualité sociale agricole, rapporte l’antenne locale de France Bleu. Le préfet, Vincent Roberti, a condamné des actions«\\xa0en dehors de tout cadre syndical\\xa0».Mais contrairement à l’action parisienne de la Confédération paysanne, aucune interpellation n’a eu lieu.',\n",
       "  'À voir également surLe HuffPost:'],\n",
       " 'publication_date': '06/12/2024 10:21',\n",
       " 'bio_sentences': ['l’un d’eux, charles fournier, élu de tours, fait remarquer à nos confrères\\xa0que d’un côté, les paysans de la conf’ paysanne sont «nassés et brutalisés lorsqu’ils protestent contre la spéculation sur les céréales\\xa0», pendant que de l’autre, il n’y a«\\xa0aucune mesure de sanction quand la fnsea déverse sa colère sur les grilles de l’office français de la biodiversité\\xa0»',\n",
       "  'fin novembre, des agriculteurs franciliens de la fnsea-ja avaient également fait ériger des murs symboliques devant l’anses et l’inrae et soudé les portes de l’office français de la biodiversité (ofb) à toulouse'],\n",
       " 'sentiment_score': 0.3779665529727936}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour enregistrer les données au format CSV\n",
    "def save_to_csv_final(data, filename):\n",
    "    \"\"\"\n",
    "    Enregistre la liste de dictionnaires au format CSV.\n",
    "    \n",
    "    :param data: Liste de dictionnaires contenant les données des articles.\n",
    "    :param filename: Nom du fichier CSV de sortie.\n",
    "    \"\"\"\n",
    "    # Définir les en-têtes en fonction des clés des dictionnaires\n",
    "    headers = [\n",
    "        \"Title\", \n",
    "        \"Summary\", \n",
    "        \"URL\", \n",
    "        \"Paragraphs\", \n",
    "        \"Date\", \n",
    "        \"Found_in\", \n",
    "        \"Average_Sentiment_Score\"\n",
    "    ]\n",
    "    \n",
    "    # Écriture dans le fichier CSV\n",
    "    with open(filename, mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        \n",
    "        # Écrire les en-têtes\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Écrire les données\n",
    "        for article in data:\n",
    "            # Extraire les données à partir du dictionnaire\n",
    "            row = {\n",
    "                \"Title\": article.get(\"title\", \"\"),\n",
    "                \"Summary\": article.get(\"description\", \"\"),\n",
    "                \"URL\": article.get(\"url\", \"\"),\n",
    "                \"Paragraphs\": \"\\n\\n\".join(article.get(\"paragraphs\", [])),  # Joindre les paragraphes par des doubles sauts de ligne\n",
    "                \"Date\": article.get(\"publication_date\", \"\"),\n",
    "                \"Found_in\": \", \".join(article.get(\"bio_sentences\", [])),  # Joindre les phrases contenant \"bio\"\n",
    "                \"Average_Sentiment_Score\": article.get(\"sentiment_score\", 0)  # Score de sentiment moyen\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Les données ont été enregistrées dans le fichier '{filename}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Fonction pour afficher les mois avec couleurs basées sur le score moyen\n",
    "def plot_months_with_color_based_on_score(data):\n",
    "    \"\"\"\n",
    "    Génère un graphique montrant le nombre d'articles par mois, avec une couleur différente pour chaque mois\n",
    "    en fonction du score moyen des articles de ce mois. Le score est extrait de la dernière colonne des données.\n",
    "    \n",
    "    :param data: Liste d'articles où la date de publication se trouve au 5ème élément et le score dans la dernière colonne.\n",
    "    \"\"\"\n",
    "    # Initialisation de dictionnaires pour compter les articles et calculer la somme des scores par mois\n",
    "    articles_per_month = {}\n",
    "    scores_per_month = {}\n",
    "\n",
    "    # Boucle pour extraire les données pertinentes\n",
    "    for entry in data:\n",
    "        date_str = entry[4]  # La date est au 5ème élément\n",
    "        score = entry[-1]  # Le score est dans la dernière colonne\n",
    "\n",
    "        try:\n",
    "            # Conversion de la date au format 'DD/MM/YYYY HH:MM'\n",
    "            date_obj = datetime.strptime(date_str, '%d/%m/%Y %H:%M')\n",
    "            \n",
    "            # Formater la date pour obtenir le mois et l'année (format 'YYYY-MM')\n",
    "            month_year = date_obj.strftime('%Y-%m')\n",
    "            \n",
    "            # Incrémenter le compteur pour le mois et ajouter le score pour le calcul du score moyen\n",
    "            if month_year not in articles_per_month:\n",
    "                articles_per_month[month_year] = 0\n",
    "                scores_per_month[month_year] = 0.0\n",
    "            articles_per_month[month_year] += 1\n",
    "            scores_per_month[month_year] += score\n",
    "        except ValueError as e:\n",
    "            print(f\"Erreur de format de date pour '{entry[0]}': {e}\")\n",
    "            continue\n",
    "\n",
    "    # Calculer le score moyen par mois\n",
    "    average_scores = {month: scores_per_month[month] / articles_per_month[month] for month in articles_per_month}\n",
    "\n",
    "    # Créer une liste de tous les mois de l'année (en format YYYY-MM)\n",
    "    all_months = pd.date_range(start=min(pd.to_datetime(list(articles_per_month.keys()))), \n",
    "                               end=max(pd.to_datetime(list(articles_per_month.keys()))), \n",
    "                               freq='MS').strftime('%Y-%m').tolist()\n",
    "\n",
    "    # Liste des mois avec ou sans articles\n",
    "    month_labels = []\n",
    "    month_counts = []\n",
    "    month_scores = []\n",
    "    \n",
    "    # Ajouter des mois manquants (ceux sans articles)\n",
    "    for month in all_months:\n",
    "        month_labels.append(month)\n",
    "        month_counts.append(articles_per_month.get(month, 0))\n",
    "        month_scores.append(average_scores.get(month, 0))\n",
    "\n",
    "    # Créer un DataFrame avec les mois, le nombre d'articles et le score moyen\n",
    "    df = pd.DataFrame({\n",
    "        'Month': month_labels,\n",
    "        'Article_Count': month_counts,\n",
    "        'Average_Score': month_scores\n",
    "    })\n",
    "\n",
    "    # Créer une échelle de couleurs pour les scores (de rouge à vert)\n",
    "    norm = mcolors.Normalize(vmin=0, vmax=1)\n",
    "    cmap = plt.colormaps['RdYlGn']  # Palette de couleurs allant de rouge (0) à vert (1)\n",
    "    \n",
    "    # Tracer les données\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    bar_colors = [cmap(norm(score)) for score in df['Average_Score']]  # Couleur des barres en fonction du score moyen\n",
    "    bars = ax.bar(df['Month'], df['Article_Count'], color=bar_colors)\n",
    "\n",
    "    # Ajouter des labels et un titre\n",
    "    ax.set_xlabel('Année', fontsize=12)\n",
    "    ax.set_ylabel('Nombre d\\'Articles', fontsize=12)\n",
    "    ax.set_title('Nombre d\\'Articles par Mois avec Scores Moyens', fontsize=14)\n",
    "\n",
    "    # Rotation des labels de l'axe X pour une meilleure lisibilité\n",
    "    plt.xticks(rotation=45, fontsize=10)\n",
    "\n",
    "    # Définir les ticks de l'axe X (positionner les mois)\n",
    "    tick_positions = range(len(df['Month']))\n",
    "    ax.set_xticks(tick_positions)\n",
    "\n",
    "    # Ne mettre l'année qu'une seule fois par année\n",
    "    ax.set_xticklabels([label if i == 0 or label[:4] != df['Month'][i-1][:4] else '' for i, label in enumerate(df['Month'])])\n",
    "\n",
    "    # Ajouter une grille\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Ajouter une légende de couleur (colorbar)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])  # Nécessaire pour afficher la légende\n",
    "    cbar = fig.colorbar(sm, ax=ax, orientation='vertical', pad=0.01)\n",
    "    cbar.set_label('Score Moyen')\n",
    "\n",
    "    # Ajustement des marges\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Afficher le graphique\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_months_with_color_based_on_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mplot_months_with_color_based_on_score\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Boucle pour extraire les données pertinentes\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 20\u001b[0m     date_str \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# La date est au 5ème élément\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     score \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Le score est dans la dernière colonne\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# Conversion de la date au format 'DD/MM/YYYY HH:MM'\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4"
     ]
    }
   ],
   "source": [
    "plot_months_with_color_based_on_score(final_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

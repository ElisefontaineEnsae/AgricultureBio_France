{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bouton 'Mon compte' détecté. Articles supplémentaires peuvent être chargés.\n"
     ]
    }
   ],
   "source": [
    "# Définir les en-têtes HTTP pour éviter d'être bloqué\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Fonction pour scraper une page de recherche et récupérer les articles\n",
    "def scrape_page(url, session):\n",
    "    all_articles = []  # Liste de tous les articles\n",
    "    seen_articles = set()  # Ensemble pour garder trace des articles déjà ajoutés\n",
    "\n",
    "    try:\n",
    "        # Envoyer une requête HTTP pour obtenir la page\n",
    "        response = session.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Trouver tous les articles pour les deux cas\n",
    "            articles = soup.find_all(\"article\", class_=[\"fig-profil fig-profil-mtpd\", \"fig-profil\"])\n",
    "\n",
    "            for article in articles:\n",
    "                # Extraire le titre\n",
    "                title_element = article.find(\"h2\", class_=\"fig-profil-headline\")\n",
    "                title_link = title_element.find(\"a\") if title_element else None\n",
    "                title_text = title_link.get_text(strip=True) if title_link else \"Titre non disponible\"\n",
    "\n",
    "                # Extraire l'URL de l'article\n",
    "                article_url = title_link[\"href\"] if title_link else None\n",
    "\n",
    "                # Extraire la description\n",
    "                description_element = article.find(\"div\", class_=\"fig-profil-chapo\")\n",
    "                description_text = description_element.get_text(strip=True) if description_element else \"Description non disponible\"\n",
    "\n",
    "                # Extraire la date de publication\n",
    "                date_element = article.find(\"li\", class_=\"fig-date-pub\")\n",
    "                time_element = date_element.find(\"time\") if date_element else None\n",
    "                date_text = time_element[\"datetime\"] if time_element else \"Date non disponible\"\n",
    "\n",
    "                # Ajouter l'article à la liste si non déjà vu\n",
    "                if article_url and article_url not in seen_articles:\n",
    "                    all_articles.append({\n",
    "                        \"Titre\": title_text,\n",
    "                        \"Description\": description_text,\n",
    "                        \"URL\": article_url,\n",
    "                        \"Date\": date_text\n",
    "                    })\n",
    "                    seen_articles.add(article_url)\n",
    "\n",
    "            # Gérer le bouton \"Voir plus de résultats\"\n",
    "            more_button = soup.find(\"button\", class_=\"fh-kw__lk fh-kw__lk--more\")\n",
    "            if more_button:\n",
    "                # Simuler un clic pour charger plus de résultats\n",
    "                next_url = more_button.get(\"data-url\")\n",
    "                if next_url:\n",
    "                    print(f\"Bouton 'Voir plus de résultats' détecté. Scraping de {next_url}\")\n",
    "                    all_articles.extend(scrape_page(next_url, session))\n",
    "            \n",
    "            # Gérer le panneau \"Mon compte\" pour charger dynamiquement les articles\n",
    "            login_button = soup.find(\"button\", class_=\"fh-user__login fh-user__login--is-connected\")\n",
    "            if login_button:\n",
    "                print(\"Bouton 'Mon compte' détecté. Articles supplémentaires peuvent être chargés.\")\n",
    "                # Simuler une interaction ou vérifier si plus de contenu s'affiche dynamiquement\n",
    "\n",
    "            return all_articles\n",
    "\n",
    "        else:\n",
    "            print(f\"Erreur HTTP {response.status_code} lors de l'accès à {url}\")\n",
    "            return []\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Erreur de requête pour {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# URL de base de la recherche\n",
    "base_url = \"https://recherche.lefigaro.fr/recherche/agriculture%20bio/\"\n",
    "\n",
    "# Initialiser une session HTTP pour gérer les requêtes\n",
    "with requests.Session() as session:\n",
    "    # Scraper la page unique\n",
    "    articles = scrape_page(base_url, session)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
